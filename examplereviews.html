<html>
   <head>
    <meta content="text/html;charset=utf-8" http-equiv="Content-Type">
    <meta content="utf-8" http-equiv="encoding">
    <title></title>
    <link rel="stylesheet" type="text/css" href="/files/css/bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="/files/css/style.css" />
    <script src="/files/js/jquery.js"></script>
    <script src="/files/js/handlebars.js"></script>
    <script src="/files/js/underscore.js"></script>
    <script src="/files/js/bootstrap.min.js"></script>
  </head>
<body>



<div class="largeheader centered splitafter">
	<div class="container">
		<div class="deco">COLUMBIA UNIVERSITY COMS 6113</div>
    <div class="title"><a href="/" style="color:black;">DATABASE RESEARCH TOPICS</a></div>
	</div>
</div>




<div class="row" style="margin-top: 2em;">
  <div class="col-md-6 col-md-offset-3">
		
<p class="review">
  <h3>---</h3>

  I think the main goal of the project is to design a new program interface to make adding new functions easier into the system. The original INGRES has been developed enough that exposes some disadvantages of earlier design decisions. Another thing is to design relational system that fits complex engineering applications such as CAD systems. It makes sense because the INGRES system has many issues in this setting: e.g. the query languages become verbose, the system lacks extendability. The last goal, to minimize the changes need to make, is quite important because it means to keep the system simply and easy-to-use. The ideas in transaction management section are most interesting to me.  The implementation of two-phase lock manager and the extension of object locking makes sense for me because it is close to the transaction management in my understanding. 

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  	The main goals of the Postgres system were to support complex data, allow new access methods, support alerts and triggers for an “active” database, and to use new technologies (such as optical disks in their memory hierarchy). I think they chose these goals partially because, as they stated, they wanted to provide functions required by CAD systems, graphics, and other popular engineering applications. I think the other part of their motivation was to see how far they could push the relational model, from an academic perspective.<br />
	By supporting user extendibility for data types and access methods, Postgres enables specific/customized functionality for different engineering requirements. The paper pushes the relational model by applying the idea of unified representation of internals and data. For example, they have a system catalog “CODE” for applications to store queries to be compiled. The code and user can be stored in the same way as data, and the only additional management required is a compilation daemon and invalidation mechanism. Similarly, their precompiled compile plans and answers are cached in relations and fields. Even locks are placed in data records themselves. The paper explains that placing locks in data records guarantees persistence and fine granularity.<br />
	The system architecture with the POSTMASTER interests me. It seems that they want the architecture to be minimal, but while reading I questioned how much functionality of the POSTMASTER would be necessary. For example, they mentioned a daemon asynchronously compile user commands, and a compilation daemon that would also utilize idle processors. Then would POSTMASTER need some scheduling algorithm, to schedule the compilation daemon and locking daemon? Also, they proposed a simple request-answer message protocol between the program and backend, but I wonder if the answer message format would need additional fields beyond response code and data requested, for example if one request has a much longer completion delay than other requests.

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  The main goals of the Postgres System were to support complex objects better, to provide users’ extendability for data types/operators/access methods, provide facilities for active databases and inferencing, simplify the crash recovery code of DBMS, make the most use of the optical disks, workstations and custom VLSI chips while making minimum changes to the relational model.<br />
<br />
I believe these goals made sense because things like users’ extendability for data types/operators/access methods or active databases and inferencing or crash recovery definitely happen or are in large demand on a daily basis. And these are also needed to be done to stay in the relational model since it is the go-to ideal data model.<br />
<br />
<br />
<br />
I find the iteration execution of queries using “*” or triggers using “always” very interesting and useful idea in this paper. Before reading the paper, I just took “always” in the trigger as granted and never knew how it was implemented. It is fascinating on how alerters or triggers are designed so that they can be awakened using the forward-chaining control structure of “always” while queries have no such awakening feature and are executed constantly until they have no effect any more. <br />
<br />
Yes, after reading the paper, I think the whole design is meticulous and thoughtful.

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  1. Main goals:<br />
1) provide better support for complex objects,<br />
WHY:<br />
Engineering data is complex and dynamic. Although the required data can be simulated on a relational database, the performance of the application is unacceptable because there may be too many queries to be executed. The support for complex objects could make the query number far less.<br />
<br />
2) provide user extendibility for data types, operators and access methods,<br />
WHY:<br />
Nowadays, many applications require specialized data types and these datatypes are simulated on the built-in data types. It could be verbose and the performance could be poor.  Such applications could be better served by allowing new data types, new operators and new access.<br />
<br />
3) provide facilities for active databases (eg, alerters and triggers) and inferencing including forward- and backward-chaining,<br />
WHY:<br />
With alerters and triggers, a lot of applications would be far easier to program, such as bug reporting system. Many expert system applications are easier to describe in rules. And many values are changing with the time. It would be better to infer them from the data rather than store them as an ordinary value and then enforce them to satisfy a collection of IC. <br />
<br />
4) simplify the DBMS code for crash recovery<br />
WHY:<br />
We should allow user-defined access recovery methods, so it is important to make the model for crash recovery to be simple and easily extendible. <br />
<br />
5) produce a design that can take advantage of new technologies<br />
WHY:<br />
Price-performance and reliability. The ability to utilize special purpose hardware demonstrates a strong case for designing and implementing custom designed VLSI.<br />
<br />
6) make as few changes as possible to the relational model<br />
WHY:<br />
Many users will become familiar with relational concepts, so this framework should not change too much.  Based on spartan simplicity, we should build a small and extensible data model instead of a large complex one.  We hope to solve everyone's problem using the relational model.<br />
<br />
Overall, I think all the six points make sense. And the author explains them in a reasonable and convincing way.<br />
<br />
2. Pick one of the (many) ideas in the paper that most interests you. <br />
<br />
I'm interested in the second goal -- to make POSTGRES easier to extend the DBMS so that it can be used in new application domains.<br />
<br />
Why is it interesting? <br />
<br />
It is interesting because it requires a super elaborate and farsighted system design. Extensibility matters the lifetime of a system.<br />
When a system just came out, it usually only support some limited features, e.g. basic data types. But if the system designers want<br />
 their system to live longer, they must guarantee that it is easy to add more functions to the system to support future applications.<br />
But this is not a simple task, especially when you are trying to make some core functions of a system to be extensible. For example, <br />
the index access methods of Postgres is extensible, so people can add new methods such as R-trees into Postgres. You can imagine the index method should interact with almost all other parts of the system -- data storage, cost model, log and recovery, etc. The designers must provide a high-level abstraction of indexing so that it can be extended easily.<br />
<br />
Does the proposed design hold water?<br />
<br />
I think the idea that providing user extendibility for data types, operators and access methods makes sense and it is very important.<br />
The proposed design holds water in the sense that the optimizer can work well with user-defined features. However, adding new access methods<br />
seems still need a lot of work. In a more recent Postgres design, it introduced GiST(Generalized Search Tree) interface to relieve this problem.<br />
I think it is a very good design. The GiST layer itself takes care of concurrency, logging and searching the tree structure, which makes users' implementing new access methods much easier.<br />
<br />


  </p>
<hr />

<p class="review">
  <h3>---</h3>

  This paper by Michael Stonebraker and Lawrence A Rowe presents the preliminary design of Postgres, a new database management system that extends the Ingres relational database system to support additional concepts. The main goals of this new system are better support for complex objects and user-defined data types, operators and access methods, support for active databases and rules (e.g. alerts and triggers), simpler crash recover and a more effective storage system. These motivations are well-grounded as at the time of writing, s conventional DBMS had a small set of built-in data types and access methods and lacked the ability to represent complex objects, which makes it inefficient for processing of user-defined data types. In addition, many modern applications need alerters and triggers to maintain data integrity. Thus, better support for those and a more general rule system ensures that data changes are correctly reflected in active databases. The design and implementation of a simpler crash recovery and storage system is discussed comparatively less by Stonebraker in this paper, but they do have their significance in the improvement of the conventional relational DBMS using the new technologies at the time.<br />
<br />
One thing that I find particularly interesting in the paper is the alerters, triggers and the general rule processing. The paper starts by giving several scenarios where alerters or triggers would be useful in maintaining a consistent active database, such as bug reporting and updates propagation. Then it brings up the idea of rule processing and touches on the approaches of forward and backward chaining. In the system architecture section, it discusses the rule firing mechanism of using physical locking of objects. Although these implementations seems pretty convincing upon first reads, deeper investigation of Stonebraker’s later papers reveals that there are limitations to this implementation, and in fact, Stonebraker has quite a few papers on going from 1986 formalizing and criticizing his previous designs of Rules. Specifically in 1991, the midterm design of Postgres [Stone91] states “It is clear to us that all DBMSs need a rules system”. In its discussion of “Implementation of Rules”, it points out that the record level processing with marking (locking) is especially efficient if there are “a large number of rules, and each covers only a few instances”. However, when each rule covers a lot of instances, it is claimed to be much more efficient to escalate individual locks on data records to a column level lock by query rewriting. The researchers were working towards exploring a rule chooser for the best implementation for each given rule. 

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  The design of POSTGRES was motivated by the fact that the conventional relational system was not suitable for many engineering applications. POSTGRES aimed to provide facilities required by those applications. They had six design goals.<br />
<br />
The first goal was to support complex objects. As it was, too many queries have to be executed to fetch data for a complex object (for example, a collection of geographic objects with display characteristics). Stonebraker and Rowe felt they needed the ability to store the object in a way that only one query is required to fetch it. This goal makes sense since it is in service of query optimization and improving performance time. <br />
<br />
The second goal was to provide user extendibility for data types, operators and access methods. It is crucial that it's implementable by non-experts, thus have easy-to-use interfaces. Many applications require specialized data types and the existing method to simulate those data types is too verbose, with poor performance. Stonebraker and Rowe also argues that b-trees are only appropriate for certain kinds of data and new access methods are often required for some data types. I thought the morivation for this goal could be made stronger is there's more evidence of this need, e.g. how many apps truly needed customizable, specialized data types?<br />
<br />
The third goal was to provide facilities for activate databases, which meant creating an alerter, trigger and support rule processing. Their motivation for this goal was to have a system in place to alert a manager about unfixed priority bugs, propagate updates within the database to maintain consistency, and set "rules" for certain data that's dependent on other factors. This goal seems immediately useful!<br />
<br />
The fourth goal was to simplify the DBMS code for crash recovery. Existing DBMS's have a large amount of crash recovery code that's difficult to test and debug. To support the goal of user-defined access methods, Stonebraker and Rowe wanted the model for crash recovery to be easily extendible and as simple as possible. Again, makes sense especially from the user's standpoint. <br />
<br />
The fifth goal was to make use of newer technologies. My question is how widespread was the availability of those newer technologies at the time? If only a small subset of the population had access, wouldn't it constraint the use of POSTGRES to the people who have those resources? Making a particular software only available to a particular group of people is inherently fine, but I do wonder if this goal to use new technology is only for newness' sake.<br />
<br />
The final goal was to make as few changes as possible to the relational model because Stonebraker and Rowe believed that users will be familiar with relational concepts and that simpler would be better. People frequently rely on heuristics to simplify decision making, so this goal makes sense.<br />
<br />
I was intrigued by the topic of Time Varying Data. The writers said one could access historical data by indicating desired time (e.g. all records for employees that worked on 7 January 1985). This seems only relevant to data that's updated daily, for example an employee punch in/punch out software. For all other applications, how is the data stored? Does the database create a snapshot of itself every day at a certain time even if no changes are made? Also, if every change to the database was tracked, wouldn't it quickly consume too much memory? 

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  Postgres in essence is an effort to start over from Ingres after early design decisions in the former precluded certain functionality from being easily implemented. As stated in the paper, the main goals are to support user implemented complex data types to enable faster access speeds in order to support a wide variety of operators, support rule processing including alerters and triggers, clean up recovery code, support new hardware while keeping the simplicity of the relational model. In a sense, Postgres aims to simultaneously expand the functionality of a system and to simplify it, which is very ambitious given that these are typically conflicting goals. They are however both necessary, the former a natural result of increased adoption and the latter a necessity for maintenance and ease of use. <br />
<br />
Typically, it is almost impossible for a program to support all possible potential use cases of the program as there are always some small detail that needs to be adjusted or changed. It is not feasible for a small team of developers to implement all changes, and the more features is added to the core application, the more conflicts and logical incompatibility exist. I am in awe at the fact that the authors of Postgres recognized this limitations from Ingres, and instead came up with a flexible interface that allowed users to modify the aspect of a DBSM to suit their needs, adding the power of generic programming while maintaining optimizations typically performed by DBSMs. The concept of having programs as a type in the form of procedures, and a separate data manipulation type POSTQUE is quite mind-blowing. Of course, given new user defined variables, optimizations must be adjusted. Since it is not possible to optimize for all user defined types, Postgres instead creates an interface that allows programmers to directly modify optimization, which vastly increases the functionality of the system at the cost of optional additional work to the users. <br />
<br />


  </p>
<hr />

<p class="review">
  <h3>---</h3>

  Postgres in essence is an effort to start over from Ingres after early design decisions in the former precluded certain functionality from being easily implemented. As stated in the paper, the main goals are to support user implemented complex data types to enable faster access speeds in order to support a wide variety of operators, support rule processing including alerters and triggers, clean up recovery code, support new hardware while keeping the simplicity of the relational model. In a sense, Postgres aims to simultaneously expand the functionality of a system and to simplify it, which is very ambitious given that these are typically conflicting goals. They are however both necessary, the former a natural result of increased adoption and the latter a necessity for maintenance and ease of use. <br />
<br />
Typically, it is almost impossible for a program to support all possible potential use cases of the program as there are always some small detail that needs to be adjusted or changed. It is not feasible for a small team of developers to implement all changes, and the more features is added to the core application, the more conflicts and logical incompatibility exist. I am in awe at the fact that the authors of Postgres recognized this limitations from Ingres, and instead came up with a flexible interface that allowed users to modify the aspect of a DBSM to suit their needs, adding the power of generic programming while maintaining optimizations typically performed by DBSMs. The concept of having programs as a type in the form of procedures, and a separate data manipulation type POSTQUE is quite mind-blowing. Of course, given new user defined variables, optimizations must be adjusted. Since it is not possible to optimize for all user defined types, Postgres instead creates an interface that allows programmers to directly modify optimization, which vastly increases the functionality of the system at the cost of optional additional work to the users. <br />
<br />


  </p>
<hr />

<p class="review">
  <h3>---</h3>

  When Postgres was written, databases were shipped as-is. End users didn’t have much potential to extend data types, create user-defined functions, or create complex objects. Changes of that level had to originate with the manufacturer.<br />
The main goal for the Postgres system, then is to create a user-extensible system. Postgres makes it possible for each end-user to customize the database, as well as future-proof their schemas by allowing extensibility. It introduces a corresponding language, POSTQUEL, that is an extended version of QUEL from Ingres. The authors chose this goal because of the limits of the Ingres and other state of the art systems: database designers had to be the ones to make meaningful changes to the system. The goal of a more customizable database is very sensible, as the authors mention, many systems operate based on rules rather than on particular values, so the need to customize a database is highly relevant. The means by which the authors propose to do this--extensible types, UDFs, and triggers--all are also very good answers to this problem. <br />
<br />
The concept of the portal that HITCHING POST defines is a very user-friendly way to access data. The buffer between the database and the user interface allows for a smooth user experience and is well thought-out. The implementation of ‘portal’ shows keen attention to the fact that the database will need to be integrated into a larger tech stack, and that the technicalities of that integration should be as hidden from the end user as possible. 

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  The 5 goals were: provide better support for complex objects, provide user extendibility for data and its respective operations, have a more interactive database, provide database crash recovery system, improve database compatibility with hardware with the least amount of changes to the current systems in place. The main issue connecting all these 5 issues is quickly using advanced data types such as shapes within a database as it would be too slow with their current methods and they also wanted improve database performance. The goal of extending functionality to advanced data types is reasonable because there is a lot of data that can’t be easily stored in a database table format (e. g. images, video, audio, etc.). This would have useful commercial (Netflix) and scientific applications (computational geometry).  With regards to database performance, this is probably a standard problem that all database researchers face. In the authors case, they had Postgres work with several CPUs which eventually is the norm for most large servers today.<br />
 <br />
The design feature of building in updates/alerts in Postgres is the most interesting feature. This is because for most businesses it is assumed there would be consistent data cleaning/updating that needs to be done. However, using a built-in automation/alert system is convenient to have. I would say the proposed design does well as the language is intuitive. Also, it is a neat feature that it supports both forward and backward chaining. 

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  The System R paper presents a relational data model which the authors implemented in two phases and which users then evaluated. They demonstrated that a model could achieve data independence of queries, while still being performant and providing complete functionality. In Phase Zero, they developed a set of SQL primitives that operated on the abstraction of data as objects/relations stored in records. The authors decided to store the system catalog in the database as regular data, allowing for user control over access policy via permission-related primitives (added later). In Phase One, they developed internal functionality of access methods, views, authorization, locking, and recovery through RSS/RDS systems.<br />
<br />
Each of Phase Zero and Phase One produced a key abstraction. The set of SQL primitives in Phase Zero hides from the user the physical representation of data, along with access paths, multiple users, faulty hardware. The RDS system, abstracts away machine-language with “access modules”, hiding, for example, OS requirements like the writable shared virtual memory. (The abstraction isn’t perfect as we see from the convoy problem, where OS problems crop up at the RDS/locking level). The RSS system provides an abstraction of three types of scans for RDS to call. The result is that the system performs well for transactions, which benefit from precompilation of access modules, and for ad hoc queries, which do not suffer from the precompilation. <br />
<br />
The evaluation between phases and in Phase Two motivate the design decisions and bolster the model. From Phase Two evaluations, the authors presented the modification used to test the optimizer, which lead to the question of the optimizer’s “repetoire” being limited. Early on, they determined that indexes would have clustering and selectivity properties, and these are inherent limitations on the optimizer’s understanding of a good path. I’m interested in how other properties could be useful—although they get the properties somewhat for free because the indexes are foremost tied to their B-tree implementation and the RSS’s index scan function. In multiple places, it seems the model is tied together carefully to achieve one big abstraction of a high-level language implemented in efficient machine-language.

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  In the convoy problem, the CPU is underutilized and performance suffers because a queue of processes requesting the same lock forms behind one particularly slow process. The paper seems to discuss two situations related to the convoy problem. Firstly, a process holding a commonly-requested lock may be preempted without releasing the lock once it consumes its allocated CPU time. While the process sleeps for a relatively long period of time, a queue of processes waiting for the same lock develops, none of which are able to make any progress while the initial process sleeps. In the second scenario, a process requests a lock multiple times within the same quanta (e.g. if it acquires, releases, and re-requests). However, if the lock is granted in order of request, the process will have to wait for all others in the existing queue before it can re-acquire the lock. As a result, a higher proportion of CPU time is spent context-switching and can even cause the system to thrash if consecutive requests to the same lock are very close together. This problem existed in the original System R design because locks were always granted to the first-in-queue. Subsequently, the locking protocol was changed so that all waiting processes have a chance at acquiring the lock once it is released. <br />
<br />
The choice of compiling SQL statements instead of relying on an interpreter was particularly impressive and well-explained in the paper. System R demonstrated that any SQL statement can be represented by some permutation of ~100 code “fragments,” a surprisingly compact library. The expressive power of this relatively small amount of code is reflective of SQL itself, which has a simple, elegantly-designed set of keywords. The use of compilation across both “precanned” and “ad hoc” environments also contributes to the elegance of the system—instead of building and supporting multiple implementations for query execution, there is a single, effective approach that is employed for all kinds of database users. The authors identified the clear performance benefits of compilation in a precanned environment with frequently-run queries. At the same time, they provide two convincing arguments for compilation in the ad hoc environment, where it is not as clear whether compiler optimizations outweigh the extra cost of compiling each single-use query: the insignificance of compilation overhead in relation to disk I/O, and the fact that compilation cost is amortized over the entire set of records fetched. <br />


  </p>
<hr />

<p class="review">
  <h3>---</h3>

  The Convoy Problem seems like it's an example of Amdahl's law -- it looks like there's a single very-high contention lock (or section of work that must be serialized), so eventually all concurrent transactions end up waiting for that lock. Another of way of saying it is that if that it takes, say, 200 ms to acquire that lock but only 10 ms for every other lock, then that long-lock will bottleneck the entire system.<br />
<br />
I think the most impressive part to me was actually the fact that that they managed to throw away Phase 0 and not get sucked into second system syndrome. Concurrency control (e.g. multiuser support) alone seems like it could turn into a second-syndrome black hole, let alone all the other features they added.<br />
<br />
Technically, I think the most interesting part was the decision to store the system catalog as a database table. It seems like a pretty clever way to re-use the durability and concurrency guarantees built into RSS with the RDS query optimizer, while also making it easier for users to introspect the state of the system.

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  The Convoy Problem refers to the case when mutiple processes contend repeatedly for high-traffic locks. Once a "popular" lock is granted to a program that undergoes a long time slice wait, all other processes requesting that lock will be waiting for that program. When Process P requests the lock after its wait, since it has just released the lock, it is forced to go to the end and wait again. Therefore, most of the cycles are spent on dispatching overhead.<br />
I am particularly impressed by the evolution of such a system. Going from phase zero to phase one, it explained many implementation details that changes over this period and explained its reasons from a user prospective. For example, how the evaluation of measure of costs, how the storage of data has changed and how extended functionalities were supported. (access control, locks, views, recovery) 

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  The Convoy Problem<br />
The Convoy Phenomenon is a problem caused by the fact that certain locks in the system are frequently requested but shortly held by many processes. Since a queue will form to use the lock, if locks are simply granted to the first waiting process in the queue, whenever the lock is released, all processes will be woken and will attempt to acquire the needed lock, but only the first in the queue will receive it. If a process releases a lock and then requests it again, that process will be added to the end of the queue. This collection of processes all waiting on the single lock is the so-called “convoy,” and it’s a problem because periodically waking up each process waiting on the lock and checking in on that needed lock for each one incurs a significant amount of overhead. This will result in a combination of wasted quanta time for the processes that cannot get the lock and in diminished use of quanta time for the process that can get the lock due to overhead (meaning that this process may need to be re-enqueued more times).<br />
<br />
System R solves this problem by allowing a process to re-request a lock and summarily receive it within the same time quantum. This avoids the wasteful nature of re-enqueuing the process. This allows all members of the queue to be more smoothly and quickly dispatched.<br />
<br />
User-Centered Design<br />
During the course of System R’s development, the team received many requests for enhancements to the system’s functionality from its experimental users. Most of these features were implemented to provide additional functionality. In addition to functioning as a proof-of-concept compiling SQL down using modular fragments of machine code, System R was molded into a user-friendly product featuring the “EXISTS” predicate, “LIKE” predicate, “PREPARE” and “EXECUTE” statements, and outer joins. The paper also elaborates on the circumstances in which users wanted to use these features, such as using the “LIKE” predicate to search for license plates beginning with “NVK.” This provides fantastic insight into some envisioned use cases of System R, and its focus on real-world usage further justifies the motivation behind creating an efficient database system in the first place. Although the system itself is designed to be efficient, the team realized that ease-of-use was a major concern, as too. This means that efficiency metrics aren't the only axis along which end users might make a database decision (ex. Python vs. C). This seems like it would be an important concern in writing any paper - what is the project meant to be used for, and, as a result, which more "intangible" metrics might be relevant?

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  System R was an impressive research and engineering effort, and the reading is a retrospective of the 6-year project. The paper discussed the phases that took place during the development process of System R, including 1) phase zero, which was more of an initial exploration of the relational model; 2) phase one, which is where they took what they learned from phase zero and based on the feedback they went on to implement a new system for the relational model, 3) and phase two, which involved the evaluation of the system.<br />
Part of the evaluation, the paper discusses “the Convoy Problem”. The convoy problem occurs when multiple processes compete for a high traffic lock that is held for a short duration in a first-come-first-serve scheduling policy. The situation that leads to the convoy problem is as follows: when a process requests a busy lock that is held by P, it gets placed into a queue with other processes that are waiting to access the lock. When P releases the lock, the locking subsystem passes the lock to the process at the head of the queue. Shortly, process P request the lock again, and this forces the process to go into the back of the queue and wait even though it still has allocated CPU time.<br />
Another issue the paper has discussed is related to the cost estimation during the optimization phase. Although they have tested the optimizer on an artificially generated dataset that follows the assumptions of the System R optimizer, they found out that even though the ordering of the predicted costs and the actual costs of different access paths was the same, the magnitude of the costs wasn’t the same in multiple cases.<br />


  </p>
<hr />

<p class="review">
  <h3>---</h3>

  The convoy problem spawns from the fact that to maintain data integrity there needs to be locks such that when one process is modifying data, the other processes can’t interact with it. In the case of System R, the locks tend to last for a very short amount of time. The convoy problem is essentially another version of resource starvation, as the process according to the paper is only allowed to acquire the lock to the system log/buffer pool once. The convoy problem happens because there is a convoy or sequence of processes that are all competing for a lock. Since the lock/unlock time is fast, the process in the convoy only have one chance to acquire the lock.<br />
I am impressed with the recovery subsystem as it is nice to know that even in 1981 the concept of creating backups was considered. The system to maintain backups through usage of shadow pages seem to have been very tedious as it seemed to have been a severe drain on computational resources and memory to keep track which file was old or new. I find the ending of this subsection particularly interesting because if I remember correctly, this is how modern databases backup their data by saving all the SQL commands that were executed. Then upon backup rerun the commands to “restore” the data to the specific point in time.<br />


  </p>
<hr />

<p class="review">
  <h3>---</h3>

  This paper discusses System R and the different phases the database iterated through while it attempted to show the benefits of the relational data model. (“A History and Evaluation of System R” 632).<br />
	“The Convoy Problem” deals with the locking subsystem of the database, which works to allow concurrency and prevent users from editing the same data at the same time. There exist high-traffic locks that are requested by most processes during their lifetime, and if these locks are held for a long time it could cause a buildup as more processes enqueue behind it and wait for the current process to yield the lock. Long processes that could hold the lock include page reading, I/O, or lock wait. (644)<br />
	The System R design prevents most of these types of waits, but it is possible that something called a “time slice wait”could occur before the process becomes able to be dispatched again, causing a buildup and a “convoy”. (644)<br />
	The problem was heightened by the fact that the system granted the lock to the queue waiting, not to the original process that still required the lock. This meant processes had to go to the end of the “convoy” and wait until it reached the front again before it could resume use of the lock. (644)<br />
	A solution was made that changed the lock release protocol of the System R design which allowed the current executing process to reacquire the lock until it finished all of its instructions. This allows any process to acquire and release a lock many times during its “time slice” and ensures that it will not be holding the lock when it goes into a long delay. This will prevent the buildup of a convoy, and allow it to move quickly if one does happen to form. (644).<br />
	Another aspect that I found particularly insightful was the decision to compile SQL statements into compact and efficient machine language routines during Phase One (636). This idea, inspired by R. Lorie’s 1976 observation, was able to vastly speed up SQL queries and allow the same queries (if the database structure is held constant) to be processed once and then run repeatedly without more processing. By breaking the query into small, machine language fragments, the compiler optimizes and breaks down the query only one time during the preprocessing step of a program. The program can then avoid the “larger and less efficient general-purpose SQL interface”. (644)<br />
This decision greatly improved the processing and speed of SQL queries when used in applications, and definitely contributed to this data model being viable and comparable in speed to previous models. (644)<br />
<br />
<br />
Astrahan, M.m. “A History and Evaluation of System R.” 1981<br />


  </p>
<hr />

<p class="review">
  <h3>---</h3>

  The Convoy Problem:<br />
<br />
What is it?<br />
<br />
It is a dispatching overhead caused by the protocol of releasing and requiring locks. In a schedule cycle, dispatching time even longer than the actual run time of processes. <br />
<br />
Why does it exist?<br />
<br />
The original lock system is a queue and only the process in the front of the queue can get it. Each process can only get a high-traffic lock once at each CPU time slides. At the middle of a quantum CPU time slide, if a process release a high-traffic lock, it can not require it again. This is because this process has been forced to go to the end of the queue.  Although the process still have CPU time, It will be blocked if it requires this lock again. Then the system will dispatch the next process more frequently which cause this overhead.<br />
<br />
How to solve it?<br />
<br />
Lock should not be granted to any particular process. So each process may request it and release it more than one time in its CPU time slide.<br />
<br />
<br />
The optimizer of System R<br />
<br />
What?<br />
<br />
The optimizer of System R is to choose a good available access path to process the query minimizing the number of page fetches. It compute estimated costs of available access paths and choose among them. The access path of System R has B-tree, scan and link.<br />
<br />
Why does it exist?<br />
<br />
The relational database system proposed by Codd supports a very high-level language in which user will not specifying algorithms for processing requests.<br />


  </p>
<hr />

<p class="review">
  <h3>---</h3>

  The locking subsystem in System R is extremely impressive. Reading this paper, it is apparent that the authors’ original gravitation toward ‘predicate locks’ is initially more intuitive than the end concept of being able to lock many sizes of subparts of the database, and trading those locks, but the latter is much more user-friendly and applicable to more use cases. The abandoning of the predicate locks both increases the modularization of different parts of the database and reduces the amount of query reconciliation that has to be done, which is certainly a good design choice. This part of the paper is both well implemented and very well explained. <br />
<br />
The convoy problem is the possibility that a process which is currently holding a high-traffic lock will go into a ‘time-slice wait’; that is, it will have used enough CPU resources that the OS puts it on the back burner for a while. After this process is put on hold, all other processes wanting to acquire that high-traffic lock will also necessarily be put on hold. This effect is due to the design choice in the VM/370 OS that each process is allocated a certain amount of CPU time after which it is put on hold.

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  The convey problem mentioned in the paper happens when the lock management policy for frequently accessed locks and operating system scheduler forces all process that requires a lock to operate sequentially, slowing the system. In a multi processing system with a frequent lock required by all processes, process that don’t process the lock wastes any cpu time assigned to it by the operating system. In the original System R design, locks operate on a first in first out queue, which forces a thread that requires a lock for more than a quanta to wait for all other threads to have a chance to hold the lock before reacquiring it, forcing it to waste all cycles in between and might have to wait out a ‘time slice wait’ before it acquires the lock again.  The main issue is that operating system does not know how long each process need a lock, and end up choosing a method that waste a lot of computing cycles. To improve from the guaranteed worst case wait time, the authors changed the lock acquisition order to a random choice each time.  <br />
<br />
Another impressive topic the paper covered is the beginning of durability in the form of a recovery subsystem. The authors analyzed the possible type of failures (entire disk, system, or individual transactions), and implemented both a limited log and replicated data in the form of ‘shadow pages’. Shadow pages are essentially updated periodically. In the event of a failure, a page reverts to the shadow page, and the log is used to undo/redo incomplete/missing transactions, which is still done today. The authors considered the effectiveness of the recovery system along both performance and scope though there was not extensive experimentation. The amount of data movement, overhead generated by the system and reliability are also considered. <br />


  </p>
<hr />

<p class="review">
  <h3>---</h3>

  Convoy Problem <br />
<br />
What is it?<br />
The convoy problem occurs when using locks for concurrency control in a multithread application. In this paper, it describes this phenomenon that a process might go into a long time slice wait while it is holding a high-traffic lock and other dispatchable processes will soon request the same lock and become queued behind the sleeping process. <br />
<br />
Why does it exist?<br />
Because there are certain high-traffic locks which every process requests frequently and holds for a short time and each process in the multiprogramming set receives a series of small quanta of CPU time. So it may hold the lock when it goes into a long time slice wait. And in the original system R, the process P releases a lock, and the lock would be granted to the first waiting process in the queue. So if P requests in the short time, it would not get the lock. Then the convoy exists. <br />
<br />
The recovery system <br />
What is it?<br />
It is a safeguard system. It combines shadow page and log mechanism in the system R.<br />
<br />
Why?<br />
 It is used to recover from system failure.<br />
In system R, the recovery system is keeping of a shadow page for each updated page. It also brings some impact to the system performance because of data moving, maintaining the version and periodic checkpoints. An alternative technique is simply to keep a log.

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  The Convoy Problem is when a process might go into a long "time slice wait" while it is holding a high traffic lock and all other dispatchable processes will soon request the same lock and become enqueued behind the sleeping process. <br />
The reason that this problem exists is due to the fact that there are certain high-traffic locks in System R which every process requests frequently and holds for a short time. <br />
<br />
Another topic that I impressed with is those extensions and improvements made to the language which arose from the suggestions of their experimental users.<br />
The reason is that most of these predicates later on have become very useful functionality in SQL and become well known to people such as EXIST and LIKE so that those experimental users somewhat had a kind of profound historical influence on technology in some way which beyond their own awareness.

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  The convoy problem is that a process might go into a long "time slice wait" while it is holding a high-traffic lock. All other threads have to queue behind the sleeping process. This is caused by the original locking subsystem design of System R. When a process P releases a lock, the locking subsystem grants the lock to the first waiting process in the queue and the lock is unavailable to P. After a short time, P once again requests the lock, and is forced to go to the end of the convoy, which would cost long time.<br />
<br />
The design of locking subsystem impressed me. Because the designers have come up with the idea “intention” lock. When locks are acquired on small objects, "intention" locks are simultaneously acquired on the larger objects which contain them. The lock solves the simulatenously updating problem.<br />


  </p>
<hr />

<p class="review">
  <h3>---</h3>

  The convoy phenomenon exists because each process receives a series of small "quanta" of CPU time. When the series of quanta ends, the process drops out of the multiprogramming set and must undergo a longer "time slice wait" before it once again becomes dispatchable. This is a problem because a process may only execute 1000 instructions and a dispatch usually takes more than 1000 instructions, which means the system ends up in a cycle of dispatching overhead.<br />
<br />
I really enjoyed reading about the Recovery Subsystem, where the developers created a means to recover the database to a consistent state in the event of failure, divided roughly into three types: disk, system or transaction failure. The mechanisms in place to manage system failure was particularly impressive, as I imagine that the developers would have had to meticulously manage the pointers between "old" and "new" pages and deal with memory allocation when each page is created (since so many copies of old pages necessarily exist, space must have been a delicately managed issue).

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  Convoy is that when multiple processes or threads acquire for a same lock (usually a high-traffic lock), they must be serialized, which is time consuming and low efficiency.<br />
In VM/370, processes are scheduled by a round-robin approach, but processes could also be blocked during their running time, and if one process is blocked, it must wait for a longer time. Therefore, there could be a situation that the process holds a high-traffic lock, which would cause other dispatchable processes enqueued behind the blocked process.<br />
The other topic I am interested is the compilation approach inspired in phase one.<br />
In phase zero, XRM uses an interpreter program to execute high-level SQL. The drawback for this method is that since the interpreter is "general-purpose" designed, it could be large in size and low in efficiency. Thus in phase one, SQL are decomposed into "fragments" first by a preprocessor.<br />
However, what interested me is that the paper did not say why the preprocessor could be smaller-size than the interpreter, given that the parsing could be the overhead and hard to implement, which is mentioned in the paper and lecture. Besides, although the paper gave the evaluation of the phase one, I am still wondering what's the cost for phase zero.

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  “System R, an experimental database system, was constructed to demonstrate that the usability advantages of the relational data model can be realized in a system with the complete function and high performance required for every data production use”. This paper describes the three principal phases of System R project and concludes that research has demonstrated the feasibility of applying a relational database system to a real production environment. They believe that the high-level user interface made possible by the relational data model can have dramatic positive effect on user productivity and data independence.<br />
<br />
One of the problems with System R raised by the paper is the “convoy phenomenon” with its locking subsystem. This happens as certain high-traffic locks in System R are frequently requested and held for a short time by every process, e.g. locks of buffer pool. Therefore, the operating system dispatcher tends to serialize all processes in the system, which resulted in underutilization of resources and thus degradation of overall performance. The paper illustrates this idea by assuming the mean time between requests for high-traffic lock to be 1000 instructions. It shows that the system goes into a “thrashing” condition because more than 1000 instructions are typically used to dispatch a process. At the end of the section, the paper also proposes a solution to the problem which involves a change to the lock release protocol.<br />
<br />
Another interesting problem that the paper identifies is the minimization of the cost (weighted sum of predicted number of I/Os and RSS calls) of processing an SQL statement. For example, this involves the selection of an optimal access path among many paths that uses different indexes or simply scans the entire relations. The paper also touches on the importance of clustering as an indication of executing order. I found this topic to be particularly interesting because prediction of I/Os and CPU costs of different access paths and query plans seems very challenging. In addition to the clustering mentioned in the paper, previous classes (4112) I took explores the order of conditions applied to the records in queries, where the optimizer aims to select a query plan that minimizes branch mispredictions to reduce CPU-related latencies. Obviously, there are many other factors the complicates this prediction. The paper does point out that although the System R optimizer was able to correctly order the access paths in the experiments, the magnitudes of the predicted costs differed from the measured costs in several cases, due to causes such as the optimizer’s inability to “predict how much data would remain in the system buffers during sorting”. This complexity and its potential solutions is what makes this topic interesting.

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  - First of all, the paper explains what is “the convoy phenomenon”: some certain high-traffic locks in System R which every process requests frequently and holds for a short time. It is a problem in System R when a process goes into long “time slice wait” while holding a “high-traffic” lock. The problem exists because of the current lock release protocol of System R. When a process is forced to the end of the convoy, it may not run enough instructions to dispatch a process which will cause a problem in System R that most of the cycles spend time dispatching overhead processes.<br />
- I am most impressed by the optimizer design of the system. The idea behind is to minimize the number of I/Os in processing SQL statements. The paper mentions benefits of selecting highly selective and better clustering indexes to minimize data fetching. Another technique is to predict the costs of join by algorithms, and then the System will choose to use the method with least cost.  

  </p>
<hr />

<p class="review">
  <h3>---</h3>

  --The paper discusses “the Convoy Problem”. Discuss the problem: What is it? Why does it exist?<br />
<br />
The Convoy Problem is a not ideal situation when certain process might be holding a high-traffic lock while go into a long “time slice wait” and therefore after a permitted number of instructions of a high-traffic lock granted to the process, the process will be forced to go to the end of the convoy, therefore creating a large overhead for dispatching. <br />
The problem exists because of the design of the lock-release protocol of System R and the fact that typically number of cycles used to dispatch a process is more than the cycles permitted for a process to hold a high-traffic lock.<br />
Therefore, it can be solved by changing the lock-release protocol of System R. By making it possible for a process to acquire and release a lock many times before it runs out of its time slice, there is an extremely high probability of the process not holding the high-traffic lock before it goes to a long “time slice wait”.<br />
<br />
--The paper discusses many many topics. Identify and pick one aspect (different than the convoy problem) that you are particularly impressed with. Discuss what and why.<br />
The paper discusses the choice of a cost measure in Phase Zero. The use of a better measure of cost that is the weighted sum of CPU time and number of “I/O”s provided a clearer view of the serious disadvantage of XRM and a better analysis of future improvements in various areas for the later phase of the project.<br />
The discovery of a better measure leads to a clearer direction of research efforts to achieve their project goals.To me, nowadays there are so many cost measures available for research and projects, but deciding and interpreting correctly on a better cost measure can really help engineers and researchers move forward and make improvements.

  </p>
<hr />


  </div>
  <div class="joiner"></div>
</div>

<div class="row footer">
<div class="col-md-4 col-md-offset-4" style="text-align: center;">
  Layout borrowed from <a href="http://cs.lmu.edu/~ray/">ray toal</a>
</div>
</div>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109213291-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109213291-1');
</script>





</body>
